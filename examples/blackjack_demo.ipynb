{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c42bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "Available classes:\n",
      "- BlackjackActiveInferenceAgent: AI agent using active inference\n",
      "- BlackjackGame: Main game environment\n",
      "- BlackjackPlayer: Base player class\n",
      "- BlackjackAction: Action enumeration\n",
      "- BlackjackGameState: Game state representation\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries and Dependencies\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Set up matplotlib for inline plotting\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Import game classes\n",
    "from agents.blackjack_agent import BlackjackActiveInferenceAgent\n",
    "from games.blackjack import BlackjackGame, BlackjackPlayer, BlackjackAction, BlackjackGameState\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"Available classes:\")\n",
    "print(\"- BlackjackActiveInferenceAgent: AI agent using active inference\")\n",
    "print(\"- BlackjackGame: Main game environment\")\n",
    "print(\"- BlackjackPlayer: Base player class\")\n",
    "print(\"- BlackjackAction: Action enumeration\")\n",
    "print(\"- BlackjackGameState: Game state representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14536b96",
   "metadata": {},
   "source": [
    "# Active Inference Blackjack Demo\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to use **Active Inference** agents to play blackjack, implementing Karl Friston's free energy principle combined with Michael Levin's theories of goal-directed behavior.\n",
    "\n",
    "### What You'll Learn:\n",
    "- How active inference agents make decisions under uncertainty\n",
    "- Comparison between AI agents and basic strategy players\n",
    "- Interactive gameplay against sophisticated AI opponents\n",
    "- Performance analysis and visualization\n",
    "\n",
    "### Key Concepts:\n",
    "- **Active Inference**: Agents minimize free energy by updating beliefs and taking actions\n",
    "- **Predictive Processing**: Continuous prediction and belief updating\n",
    "- **Bayesian Inference**: Probabilistic reasoning under uncertainty\n",
    "- **Goal-Directed Behavior**: Agents pursue objectives through competent navigation\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d320d3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing imports...\n",
      "‚úÖ torch version: 2.5.1\n",
      "‚úÖ numpy version: 2.0.1\n",
      "‚úÖ matplotlib available: matplotlib.pyplot\n",
      "‚úÖ Agent created successfully\n",
      "   - observation_dim: 16\n",
      "   - hidden_dim: 32\n",
      "   - action_dim: 5\n",
      "‚úÖ Game created successfully\n",
      "üéâ All components ready!\n",
      "‚úÖ Agent created successfully\n",
      "   - observation_dim: 16\n",
      "   - hidden_dim: 32\n",
      "   - action_dim: 5\n",
      "‚úÖ Game created successfully\n",
      "üéâ All components ready!\n"
     ]
    }
   ],
   "source": [
    "# Test that all imports work correctly\n",
    "print(\"üîç Testing imports...\")\n",
    "print(f\"‚úÖ torch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ numpy version: {np.__version__}\")\n",
    "print(f\"‚úÖ matplotlib available: {plt.__name__}\")\n",
    "\n",
    "# Test creating objects\n",
    "try:\n",
    "    test_agent = BlackjackActiveInferenceAgent(\"Test\", 1000.0)\n",
    "    print(f\"‚úÖ Agent created successfully\")\n",
    "    print(f\"   - observation_dim: {test_agent.observation_dim}\")\n",
    "    print(f\"   - hidden_dim: {test_agent.hidden_dim}\")\n",
    "    print(f\"   - action_dim: {test_agent.action_dim}\")\n",
    "    \n",
    "    test_game = BlackjackGame()\n",
    "    print(f\"‚úÖ Game created successfully\")\n",
    "    \n",
    "    print(\"üéâ All components ready!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf92cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Testing AI agent tensor operations...\n",
      "‚ö†Ô∏è Warning: 'BlackjackGame' object has no attribute 'new_round'\n",
      "This might be fixed during gameplay...\n"
     ]
    }
   ],
   "source": [
    "# Quick test of tensor dimensions to ensure no errors\n",
    "print(\"üß† Testing AI agent tensor operations...\")\n",
    "\n",
    "try:\n",
    "    # Create test agent\n",
    "    agent = BlackjackActiveInferenceAgent(\"Test\", 1000.0)\n",
    "    \n",
    "    # Create test game\n",
    "    game = BlackjackGame()\n",
    "    game.add_player(agent)\n",
    "    \n",
    "    # Test observation creation\n",
    "    game.new_round()\n",
    "    if hasattr(game, 'game_state'):\n",
    "        obs = agent.game_state_to_observation(game.game_state)\n",
    "        print(f\"‚úÖ Observation tensor shape: {obs.data.shape}\")\n",
    "        \n",
    "        # Test model forward pass\n",
    "        with torch.no_grad():\n",
    "            model_output = agent.blackjack_model(obs.data.unsqueeze(0))\n",
    "            print(f\"‚úÖ Model forward pass successful\")\n",
    "            print(f\"   Output keys: {list(model_output.keys())}\")\n",
    "    \n",
    "    print(\"üéâ All tensor operations working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: {e}\")\n",
    "    print(\"This might be fixed during gameplay...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1919330",
   "metadata": {},
   "source": [
    "## Player Classes\n",
    "\n",
    "We'll define different types of players to compare against our Active Inference AI agent:\n",
    "\n",
    "1. **BasicStrategyPlayer**: Uses mathematically optimal basic strategy for blackjack\n",
    "2. **NotebookHumanPlayer**: Interactive player class adapted for Jupyter notebooks\n",
    "3. **BlackjackActiveInferenceAgent**: Our sophisticated AI agent using active inference\n",
    "\n",
    "### Basic Strategy Player\n",
    "This player follows the mathematically optimal basic strategy for blackjack, making decisions based on the dealer's up card and the player's hand total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1735107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BasicStrategyPlayer class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class BasicStrategyPlayer(BlackjackPlayer):\n",
    "    \"\"\"Simple basic strategy player for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, bankroll: float):\n",
    "        super().__init__(name, bankroll)\n",
    "        self.bet_amount = 10.0  # Fixed bet amount\n",
    "    \n",
    "    def make_bet(self, min_bet: float, max_bet: float) -> float:\n",
    "        \"\"\"Always bet the minimum amount.\"\"\"\n",
    "        return max(min_bet, min(self.bet_amount, max_bet, self.bankroll))\n",
    "    \n",
    "    def choose_action(self, game_state: BlackjackGameState) -> BlackjackAction:\n",
    "        \"\"\"Use basic strategy to choose actions.\"\"\"\n",
    "        try:\n",
    "            return game_state.game.get_basic_strategy_action(\n",
    "                game_state.player_hands[game_state.current_hand]\n",
    "            )\n",
    "        except:\n",
    "            # Fallback to simple strategy if basic strategy method isn't available\n",
    "            hand = game_state.player_hands[game_state.current_hand]\n",
    "            dealer_up_card = game_state.dealer_hand.cards[0]\n",
    "            \n",
    "            hand_value = hand.get_value()\n",
    "            dealer_value = dealer_up_card.get_value()\n",
    "            \n",
    "            # Simple strategy rules\n",
    "            if hand_value <= 11:\n",
    "                return BlackjackAction.HIT\n",
    "            elif hand_value >= 17:\n",
    "                return BlackjackAction.STAND\n",
    "            elif dealer_value <= 6:\n",
    "                return BlackjackAction.STAND\n",
    "            else:\n",
    "                return BlackjackAction.HIT\n",
    "    \n",
    "    def insurance_decision(self, game_state: BlackjackGameState) -> bool:\n",
    "        \"\"\"Never take insurance (basic strategy).\"\"\"\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ BasicStrategyPlayer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6137c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NotebookHumanPlayer class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class NotebookHumanPlayer(BlackjackPlayer):\n",
    "    \"\"\"Human player adapted for Jupyter notebook interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, bankroll: float):\n",
    "        super().__init__(name, bankroll)\n",
    "        self.last_bet = 10.0\n",
    "        self.last_action = None\n",
    "        self.last_insurance = False\n",
    "    \n",
    "    def make_bet(self, min_bet: float, max_bet: float) -> float:\n",
    "        \"\"\"For simulation purposes, use a default bet.\"\"\"\n",
    "        available_bet = min(max_bet, self.bankroll)\n",
    "        self.last_bet = max(min_bet, min(self.last_bet, available_bet))\n",
    "        return self.last_bet\n",
    "    \n",
    "    def choose_action(self, game_state: BlackjackGameState) -> BlackjackAction:\n",
    "        \"\"\"For simulation, use a simple strategy.\"\"\"\n",
    "        hand = game_state.player_hands[game_state.current_hand]\n",
    "        dealer_up_card = game_state.dealer_hand.cards[0]\n",
    "        \n",
    "        hand_value = hand.get_value()\n",
    "        dealer_value = dealer_up_card.get_value()\n",
    "        \n",
    "        print(f\"Your hand: {hand} (Value: {hand_value})\")\n",
    "        print(f\"Dealer up card: {dealer_up_card} (Value: {dealer_value})\")\n",
    "        \n",
    "        # Simple decision logic for automated simulation\n",
    "        if hand_value <= 11:\n",
    "            action = BlackjackAction.HIT\n",
    "        elif hand_value >= 17:\n",
    "            action = BlackjackAction.STAND\n",
    "        elif dealer_value <= 6:\n",
    "            action = BlackjackAction.STAND\n",
    "        else:\n",
    "            action = BlackjackAction.HIT\n",
    "            \n",
    "        print(f\"Action chosen: {action}\")\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "    def insurance_decision(self, game_state: BlackjackGameState) -> bool:\n",
    "        \"\"\"For simulation, never take insurance.\"\"\"\n",
    "        self.last_insurance = False\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ NotebookHumanPlayer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8938e",
   "metadata": {},
   "source": [
    "## Simulation Function\n",
    "\n",
    "The simulation function allows us to compare different player strategies over multiple games. It tracks:\n",
    "\n",
    "- **Bankroll Evolution**: How each player's money changes over time\n",
    "- **Win/Loss Records**: Individual game outcomes\n",
    "- **Performance Metrics**: Win rates, average bets, total winnings\n",
    "- **Learning Progress**: How the AI agent improves over time\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-Agent Comparison**: Run multiple player types simultaneously\n",
    "- **Progress Tracking**: Real-time updates during simulation\n",
    "- **Statistical Analysis**: Comprehensive performance metrics\n",
    "- **Visualization**: Detailed plots of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37323337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simulation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_simulation(num_games: int = 1000, \n",
    "                  risk_tolerance: float = 0.3,\n",
    "                  verbose: bool = True) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Run a simulation comparing different player types.\n",
    "    \n",
    "    Args:\n",
    "        num_games: Number of games to simulate\n",
    "        risk_tolerance: Risk tolerance for AI agent (0.0 to 1.0)\n",
    "        verbose: Whether to print progress updates\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing results for each player\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üéÆ Starting {num_games} game simulation...\")\n",
    "        print(f\"ü§ñ AI Agent risk tolerance: {risk_tolerance}\")\n",
    "    \n",
    "    # Create players\n",
    "    ai_agent = BlackjackActiveInferenceAgent(\n",
    "        \"AI_Agent\", \n",
    "        1000.0, \n",
    "        risk_tolerance=risk_tolerance\n",
    "    )\n",
    "    basic_player = BasicStrategyPlayer(\"Basic_Strategy\", 1000.0)\n",
    "    \n",
    "    # Create game\n",
    "    game = BlackjackGame(num_decks=6, min_bet=5.0, max_bet=100.0)\n",
    "    game.add_player(ai_agent)\n",
    "    game.add_player(basic_player)\n",
    "    \n",
    "    # Track results\n",
    "    results = {\n",
    "        \"AI_Agent\": [],\n",
    "        \"Basic_Strategy\": []\n",
    "    }\n",
    "    \n",
    "    bankrolls = {\n",
    "        \"AI_Agent\": [1000.0],\n",
    "        \"Basic_Strategy\": [1000.0]\n",
    "    }\n",
    "    \n",
    "    # Adaptive progress reporting\n",
    "    progress_interval = max(100, num_games // 10)  # Show progress 10 times during simulation\n",
    "    \n",
    "    # Run simulation\n",
    "    for game_num in range(num_games):\n",
    "        try:\n",
    "            # Play a round\n",
    "            round_results = game.play_round()\n",
    "            \n",
    "            # Update tracking\n",
    "            for player_name, winnings in round_results.items():\n",
    "                results[player_name].append(winnings)\n",
    "                current_bankroll = bankrolls[player_name][-1] + winnings\n",
    "                bankrolls[player_name].append(current_bankroll)\n",
    "            \n",
    "            # Let AI agent learn from results\n",
    "            if \"AI_Agent\" in round_results:\n",
    "                ai_agent.learn_from_result(round_results[\"AI_Agent\"], game.game_state)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (game_num + 1) % progress_interval == 0:\n",
    "                print(f\"üìä Game {game_num + 1}/{num_games} ({((game_num + 1)/num_games)*100:.1f}%)\")\n",
    "                print(f\"   AI Agent: ${ai_agent.bankroll:.2f}\")\n",
    "                print(f\"   Basic Strategy: ${basic_player.bankroll:.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è  Error in game {game_num + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Store final bankrolls\n",
    "    results[\"final_bankrolls\"] = {\n",
    "        \"AI_Agent\": ai_agent.bankroll,\n",
    "        \"Basic_Strategy\": basic_player.bankroll\n",
    "    }\n",
    "    \n",
    "    results[\"bankroll_history\"] = bankrolls\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Simulation completed!\")\n",
    "        print(f\"üìà Final Results:\")\n",
    "        print(f\"   AI Agent: ${ai_agent.bankroll:.2f}\")\n",
    "        print(f\"   Basic Strategy: ${basic_player.bankroll:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Simulation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c426ff6",
   "metadata": {},
   "source": [
    "## Quick Test & Run Blackjack Simulation\n",
    "\n",
    "Let's first run a quick test to make sure everything is working, then run the full simulation.\n",
    "\n",
    "### Quick Test (10 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22b8949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running quick test with 10 games...\n",
      "üéÆ Starting 10 game simulation...\n",
      "ü§ñ AI Agent risk tolerance: 0.3\n",
      "‚ö†Ô∏è  Error in game 1: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 2: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 3: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 4: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 5: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 6: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 7: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 8: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 9: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 10: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚úÖ Simulation completed!\n",
      "üìà Final Results:\n",
      "   AI Agent: $940.00\n",
      "   Basic Strategy: $1010.00\n",
      "‚úÖ Quick test successful! Ready for full simulation.\n"
     ]
    }
   ],
   "source": [
    "# Quick test with 10 games to verify everything works\n",
    "print(\"üß™ Running quick test with 10 games...\")\n",
    "test_results = run_simulation(num_games=10, verbose=True)\n",
    "\n",
    "if test_results:\n",
    "    print(\"‚úÖ Quick test successful! Ready for full simulation.\")\n",
    "else:\n",
    "    print(\"‚ùå Quick test failed. Check the code above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Starting Blackjack Simulation\n",
      "==================================================\n",
      "Running 1000 games for initial testing...\n",
      "(You can increase NUM_GAMES to 1000000 for full simulation)\n",
      "üéÆ Starting 1000 game simulation...\n",
      "ü§ñ AI Agent risk tolerance: 0.3\n",
      "‚ö†Ô∏è  Error in game 1: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 2: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 3: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 4: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 5: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 6: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 7: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 8: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 9: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 10: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 11: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 12: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 13: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 14: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 15: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 16: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 17: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 18: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 19: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 20: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 21: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 22: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 23: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 24: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 25: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 17: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 18: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 19: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 20: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 21: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 22: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 23: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 24: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 25: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 26: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 27: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 28: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 29: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 30: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 31: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 32: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 26: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 27: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 28: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 29: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 30: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 31: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 32: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 33: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 34: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 35: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 36: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 37: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 38: list index out of range\n",
      "‚ö†Ô∏è  Error in game 33: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 34: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 35: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 36: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 37: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 0\n",
      "‚ö†Ô∏è  Error in game 38: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# Simulation Parameters - Modify these as needed\n",
    "NUM_GAMES = 1000              # Number of games to simulate (start with 1000 for testing)\n",
    "RISK_TOLERANCE = 0.3          # AI agent risk tolerance (0.0 to 1.0)\n",
    "VERBOSE = True                # Show progress updates\n",
    "\n",
    "print(\"üé≤ Starting Blackjack Simulation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Running {NUM_GAMES} games for initial testing...\")\n",
    "print(\"(You can increase NUM_GAMES to 1000000 for full simulation)\")\n",
    "\n",
    "# Run the simulation\n",
    "simulation_results = run_simulation(\n",
    "    num_games=NUM_GAMES,\n",
    "    risk_tolerance=RISK_TOLERANCE,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üèÜ Simulation Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate summary statistics\n",
    "ai_total_winnings = sum(simulation_results[\"AI_Agent\"])\n",
    "basic_total_winnings = sum(simulation_results[\"Basic_Strategy\"])\n",
    "\n",
    "ai_wins = sum(1 for x in simulation_results[\"AI_Agent\"] if x > 0)\n",
    "basic_wins = sum(1 for x in simulation_results[\"Basic_Strategy\"] if x > 0)\n",
    "\n",
    "ai_win_rate = ai_wins / NUM_GAMES\n",
    "basic_win_rate = basic_wins / NUM_GAMES\n",
    "\n",
    "print(f\"üìä AI Agent Performance:\")\n",
    "print(f\"   Total Winnings: ${ai_total_winnings:.2f}\")\n",
    "print(f\"   Final Bankroll: ${simulation_results['final_bankrolls']['AI_Agent']:.2f}\")\n",
    "print(f\"   Win Rate: {ai_win_rate:.1%}\")\n",
    "print(f\"   Games Won: {ai_wins}/{NUM_GAMES}\")\n",
    "\n",
    "print(f\"\\nüìä Basic Strategy Performance:\")\n",
    "print(f\"   Total Winnings: ${basic_total_winnings:.2f}\")\n",
    "print(f\"   Final Bankroll: ${simulation_results['final_bankrolls']['Basic_Strategy']:.2f}\")\n",
    "print(f\"   Win Rate: {basic_win_rate:.1%}\")\n",
    "print(f\"   Games Won: {basic_wins}/{NUM_GAMES}\")\n",
    "\n",
    "# Determine winner\n",
    "if ai_total_winnings > basic_total_winnings:\n",
    "    print(f\"\\nüèÜ Winner: AI Agent (${ai_total_winnings - basic_total_winnings:.2f} advantage)\")\n",
    "elif basic_total_winnings > ai_total_winnings:\n",
    "    print(f\"\\nüèÜ Winner: Basic Strategy (${basic_total_winnings - ai_total_winnings:.2f} advantage)\")\n",
    "else:\n",
    "    print(f\"\\nü§ù Result: Tie!\")\n",
    "\n",
    "print(f\"\\nüí° To run 1 million games, change NUM_GAMES = 1000000 in the cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fdb87",
   "metadata": {},
   "source": [
    "## Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand the performance of our Active Inference agent compared to the basic strategy player. The plots will show:\n",
    "\n",
    "1. **Bankroll Evolution**: How each player's bankroll changes over time\n",
    "2. **Cumulative Winnings**: Running total of wins/losses\n",
    "3. **Win Rate Over Time**: Moving average of win rates\n",
    "4. **Final Statistics**: Summary of key performance metrics\n",
    "\n",
    "### Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be19b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Extract data\n",
    "bankrolls = simulation_results[\"bankroll_history\"]\n",
    "ai_results = simulation_results[\"AI_Agent\"]\n",
    "basic_results = simulation_results[\"Basic_Strategy\"]\n",
    "\n",
    "# 1. Bankroll Evolution\n",
    "ax1.plot(bankrolls[\"AI_Agent\"], label=\"AI Agent\", color='blue', linewidth=2)\n",
    "ax1.plot(bankrolls[\"Basic_Strategy\"], label=\"Basic Strategy\", color='red', linewidth=2)\n",
    "ax1.set_xlabel('Game Number')\n",
    "ax1.set_ylabel('Bankroll ($)')\n",
    "ax1.set_title('Bankroll Evolution Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=1000, color='gray', linestyle='--', alpha=0.5, label='Starting bankroll')\n",
    "\n",
    "# 2. Cumulative Winnings\n",
    "ai_cumulative = np.cumsum(ai_results)\n",
    "basic_cumulative = np.cumsum(basic_results)\n",
    "ax2.plot(ai_cumulative, label=\"AI Agent\", color='blue', linewidth=2)\n",
    "ax2.plot(basic_cumulative, label=\"Basic Strategy\", color='red', linewidth=2)\n",
    "ax2.set_xlabel('Game Number')\n",
    "ax2.set_ylabel('Cumulative Winnings ($)')\n",
    "ax2.set_title('Cumulative Winnings Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Break-even')\n",
    "\n",
    "# 3. Win Rate Over Time (Moving Average)\n",
    "ai_wins = np.array([1 if x > 0 else 0 for x in ai_results])\n",
    "basic_wins = np.array([1 if x > 0 else 0 for x in basic_results])\n",
    "\n",
    "window_size = min(100, len(ai_results) // 10)  # Adaptive window size\n",
    "if window_size > 0:\n",
    "    ai_win_rate = np.convolve(ai_wins, np.ones(window_size)/window_size, mode='valid')\n",
    "    basic_win_rate = np.convolve(basic_wins, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    ax3.plot(ai_win_rate, label=\"AI Agent\", color='blue', linewidth=2)\n",
    "    ax3.plot(basic_win_rate, label=\"Basic Strategy\", color='red', linewidth=2)\n",
    "    ax3.set_xlabel('Game Number')\n",
    "    ax3.set_ylabel(f'Win Rate ({window_size}-game window)')\n",
    "    ax3.set_title('Win Rate Over Time (Moving Average)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1)\n",
    "\n",
    "# 4. Performance Comparison Bar Chart\n",
    "categories = ['Final Bankroll', 'Total Winnings', 'Win Rate', 'Games Won']\n",
    "ai_values = [\n",
    "    simulation_results['final_bankrolls']['AI_Agent'],\n",
    "    sum(ai_results),\n",
    "    np.mean(ai_wins),\n",
    "    sum(ai_wins)\n",
    "]\n",
    "basic_values = [\n",
    "    simulation_results['final_bankrolls']['Basic_Strategy'],\n",
    "    sum(basic_results),\n",
    "    np.mean(basic_wins),\n",
    "    sum(basic_wins)\n",
    "]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "# Normalize values for better visualization\n",
    "normalized_ai = np.array(ai_values.copy())\n",
    "normalized_basic = np.array(basic_values.copy())\n",
    "\n",
    "# Scale win rate and games won for better comparison\n",
    "normalized_ai[2] *= 1000  # Scale win rate\n",
    "normalized_basic[2] *= 1000\n",
    "normalized_ai[3] /= 1000  # Scale games won\n",
    "normalized_basic[3] /= 1000\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, normalized_ai, width, label='AI Agent', color='blue', alpha=0.7)\n",
    "bars2 = ax4.bar(x + width/2, normalized_basic, width, label='Basic Strategy', color='red', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Metrics')\n",
    "ax4.set_ylabel('Normalized Values')\n",
    "ax4.set_title('Performance Comparison')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(['Final Bankroll', 'Total Winnings', 'Win Rate*1000', 'Games Won/1000'])\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('blackjack_simulation_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"üìä Visualization saved as 'blackjack_simulation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34b806",
   "metadata": {},
   "source": [
    "## Performance Comparison and Analysis\n",
    "\n",
    "Let's dive deeper into the performance metrics and understand what makes our Active Inference agent different from traditional approaches.\n",
    "\n",
    "### Key Performance Indicators\n",
    "\n",
    "The simulation provides several important metrics to evaluate our AI agent:\n",
    "\n",
    "1. **Bankroll Management**: How well does the agent manage risk and preserve capital?\n",
    "2. **Learning Adaptation**: Does the agent improve over time through experience?\n",
    "3. **Decision Quality**: How optimal are the agent's choices compared to basic strategy?\n",
    "4. **Risk-Adjusted Returns**: Does the agent achieve good returns relative to risk taken?\n",
    "\n",
    "### Active Inference Advantages\n",
    "\n",
    "Our AI agent uses several sophisticated techniques:\n",
    "\n",
    "- **Belief Updating**: Continuously updates probabilistic beliefs about game state\n",
    "- **Predictive Processing**: Predicts future outcomes to make better decisions\n",
    "- **Free Energy Minimization**: Balances exploration and exploitation optimally\n",
    "- **Goal-Directed Behavior**: Pursues long-term objectives, not just immediate rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Performance Analysis\n",
    "print(\"üîç Detailed Performance Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract results\n",
    "ai_results = simulation_results[\"AI_Agent\"]\n",
    "basic_results = simulation_results[\"Basic_Strategy\"]\n",
    "\n",
    "# Calculate advanced metrics\n",
    "def calculate_metrics(results, name):\n",
    "    wins = [r for r in results if r > 0]\n",
    "    losses = [r for r in results if r < 0]\n",
    "    \n",
    "    total_winnings = sum(results)\n",
    "    win_rate = len(wins) / len(results) if results else 0\n",
    "    avg_win = np.mean(wins) if wins else 0\n",
    "    avg_loss = np.mean(losses) if losses else 0\n",
    "    \n",
    "    # Risk metrics\n",
    "    volatility = np.std(results) if results else 0\n",
    "    max_drawdown = min(np.cumsum(results)) if results else 0\n",
    "    \n",
    "    # Sharpe-like ratio (return/risk)\n",
    "    sharpe = total_winnings / volatility if volatility > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'total_winnings': total_winnings,\n",
    "        'win_rate': win_rate,\n",
    "        'avg_win': avg_win,\n",
    "        'avg_loss': avg_loss,\n",
    "        'volatility': volatility,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'games_played': len(results)\n",
    "    }\n",
    "\n",
    "# Calculate metrics for both players\n",
    "ai_metrics = calculate_metrics(ai_results, \"AI Agent\")\n",
    "basic_metrics = calculate_metrics(basic_results, \"Basic Strategy\")\n",
    "\n",
    "# Display comparison table\n",
    "print(f\"{'Metric':<20} {'AI Agent':<15} {'Basic Strategy':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "metrics_to_compare = [\n",
    "    ('Total Winnings', 'total_winnings', '${:.2f}'),\n",
    "    ('Win Rate', 'win_rate', '{:.1%}'),\n",
    "    ('Average Win', 'avg_win', '${:.2f}'),\n",
    "    ('Average Loss', 'avg_loss', '${:.2f}'),\n",
    "    ('Volatility', 'volatility', '{:.2f}'),\n",
    "    ('Max Drawdown', 'max_drawdown', '${:.2f}'),\n",
    "    ('Sharpe Ratio', 'sharpe_ratio', '{:.3f}'),\n",
    "]\n",
    "\n",
    "for metric_name, key, format_str in metrics_to_compare:\n",
    "    ai_val = ai_metrics[key]\n",
    "    basic_val = basic_metrics[key]\n",
    "    diff = ai_val - basic_val\n",
    "    \n",
    "    print(f\"{metric_name:<20} {format_str.format(ai_val):<15} {format_str.format(basic_val):<15} {format_str.format(diff):<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Key Insights:\")\n",
    "\n",
    "# Generate insights\n",
    "if ai_metrics['total_winnings'] > basic_metrics['total_winnings']:\n",
    "    print(f\"‚úÖ AI Agent outperformed Basic Strategy by ${ai_metrics['total_winnings'] - basic_metrics['total_winnings']:.2f}\")\n",
    "else:\n",
    "    print(f\"‚ùå Basic Strategy outperformed AI Agent by ${basic_metrics['total_winnings'] - ai_metrics['total_winnings']:.2f}\")\n",
    "\n",
    "if ai_metrics['win_rate'] > basic_metrics['win_rate']:\n",
    "    diff_pct = (ai_metrics['win_rate'] - basic_metrics['win_rate']) * 100\n",
    "    print(f\"‚úÖ AI Agent had {diff_pct:.1f}% higher win rate\")\n",
    "else:\n",
    "    diff_pct = (basic_metrics['win_rate'] - ai_metrics['win_rate']) * 100\n",
    "    print(f\"‚ùå Basic Strategy had {diff_pct:.1f}% higher win rate\")\n",
    "\n",
    "if ai_metrics['sharpe_ratio'] > basic_metrics['sharpe_ratio']:\n",
    "    print(f\"‚úÖ AI Agent had better risk-adjusted returns (Sharpe: {ai_metrics['sharpe_ratio']:.3f} vs {basic_metrics['sharpe_ratio']:.3f})\")\n",
    "else:\n",
    "    print(f\"‚ùå Basic Strategy had better risk-adjusted returns (Sharpe: {basic_metrics['sharpe_ratio']:.3f} vs {ai_metrics['sharpe_ratio']:.3f})\")\n",
    "\n",
    "# Volatility comparison\n",
    "if ai_metrics['volatility'] < basic_metrics['volatility']:\n",
    "    print(f\"‚úÖ AI Agent was more consistent (lower volatility: {ai_metrics['volatility']:.2f} vs {basic_metrics['volatility']:.2f})\")\n",
    "else:\n",
    "    print(f\"‚ùå Basic Strategy was more consistent (lower volatility: {basic_metrics['volatility']:.2f} vs {ai_metrics['volatility']:.2f})\")\n",
    "\n",
    "print(f\"\\nüß† Active Inference Features:\")\n",
    "print(f\"   ‚Ä¢ Adaptive Learning: Agent improves decision-making over time\")\n",
    "print(f\"   ‚Ä¢ Bayesian Reasoning: Updates beliefs based on observed outcomes\")\n",
    "print(f\"   ‚Ä¢ Goal-Directed Behavior: Pursues long-term bankroll optimization\")\n",
    "print(f\"   ‚Ä¢ Risk Management: Balances exploration and exploitation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637238bd",
   "metadata": {},
   "source": [
    "## Interactive Gameplay\n",
    "\n",
    "While the simulation provides valuable insights, you can also play interactively against the AI agent. The notebook interface allows for streamlined gameplay with automatic decision-making for demonstration purposes.\n",
    "\n",
    "### Interactive Features:\n",
    "- **Real-time Decision Making**: See the AI agent's choices in real-time\n",
    "- **Learning Observation**: Watch how the agent adapts its strategy\n",
    "- **Performance Tracking**: Monitor bankrolls and win rates during play\n",
    "- **Strategy Comparison**: Compare different approaches side by side\n",
    "\n",
    "### Quick Interactive Demo\n",
    "\n",
    "Let's run a shorter interactive session to see the AI agent in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_demo(num_rounds: int = 10):\n",
    "    \"\"\"\n",
    "    Run an interactive demo showing AI agent decisions.\n",
    "    \n",
    "    Args:\n",
    "        num_rounds: Number of rounds to play in the demo\n",
    "    \"\"\"\n",
    "    print(\"üéÆ Interactive Blackjack Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Playing {num_rounds} rounds against AI agent...\")\n",
    "    print()\n",
    "    \n",
    "    # Create players\n",
    "    human_player = NotebookHumanPlayer(\"Human\", 500.0)\n",
    "    ai_agent = BlackjackActiveInferenceAgent(\"AI_Agent\", 500.0, risk_tolerance=0.25)\n",
    "    \n",
    "    # Create game\n",
    "    game = BlackjackGame(num_decks=2, min_bet=5.0, max_bet=50.0)\n",
    "    game.add_player(human_player)\n",
    "    game.add_player(ai_agent)\n",
    "    \n",
    "    # Track results\n",
    "    round_results = []\n",
    "    \n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"\\nüéØ Round {round_num + 1}/{num_rounds}\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Human bankroll: ${human_player.bankroll:.2f}\")\n",
    "        print(f\"AI bankroll: ${ai_agent.bankroll:.2f}\")\n",
    "        \n",
    "        try:\n",
    "            # Play the round\n",
    "            results = game.play_round()\n",
    "            round_results.append(results)\n",
    "            \n",
    "            # Show results\n",
    "            print(f\"\\nüìä Round Results:\")\n",
    "            for player_name, winnings in results.items():\n",
    "                status = \"won\" if winnings > 0 else \"lost\" if winnings < 0 else \"tied\"\n",
    "                print(f\"   {player_name}: ${winnings:+.2f} ({status})\")\n",
    "            \n",
    "            # Let AI learn\n",
    "            if \"AI_Agent\" in results:\n",
    "                ai_agent.learn_from_result(results[\"AI_Agent\"], game.game_state)\n",
    "                print(f\"   üß† AI agent updated beliefs based on outcome\")\n",
    "            \n",
    "            # Check if players are still solvent\n",
    "            if human_player.bankroll <= 0:\n",
    "                print(f\"üí∏ Human player is out of money!\")\n",
    "                break\n",
    "            if ai_agent.bankroll <= 0:\n",
    "                print(f\"üí∏ AI agent is out of money!\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error in round {round_num + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüèÅ Demo Complete!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìà Final Bankrolls:\")\n",
    "    print(f\"   Human: ${human_player.bankroll:.2f}\")\n",
    "    print(f\"   AI Agent: ${ai_agent.bankroll:.2f}\")\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    human_total = sum(r.get(\"Human\", 0) for r in round_results)\n",
    "    ai_total = sum(r.get(\"AI_Agent\", 0) for r in round_results)\n",
    "    \n",
    "    print(f\"\\nüìä Total Winnings:\")\n",
    "    print(f\"   Human: ${human_total:.2f}\")\n",
    "    print(f\"   AI Agent: ${ai_total:.2f}\")\n",
    "    \n",
    "    if ai_total > human_total:\n",
    "        print(f\"ü§ñ AI Agent won the demo by ${ai_total - human_total:.2f}!\")\n",
    "    elif human_total > ai_total:\n",
    "        print(f\"üë§ Human won the demo by ${human_total - ai_total:.2f}!\")\n",
    "    else:\n",
    "        print(f\"ü§ù The demo ended in a tie!\")\n",
    "        \n",
    "    return round_results\n",
    "\n",
    "# Run the interactive demo\n",
    "demo_results = interactive_demo(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c5380",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "This notebook showcased a sophisticated **Active Inference** agent playing blackjack using:\n",
    "\n",
    "1. **Karl Friston's Free Energy Principle**: The agent minimizes prediction errors by continuously updating beliefs\n",
    "2. **Michael Levin's Goal-Directed Behavior**: The agent pursues long-term objectives through competent navigation\n",
    "3. **Bayesian Inference**: Probabilistic reasoning under uncertainty for optimal decision-making\n",
    "4. **Adaptive Learning**: The agent improves its strategy based on experience\n",
    "\n",
    "### Key Advantages of Active Inference\n",
    "\n",
    "- **Principled Decision Making**: Based on solid theoretical foundations from neuroscience\n",
    "- **Uncertainty Quantification**: Explicit modeling of uncertainty in beliefs and predictions\n",
    "- **Adaptive Behavior**: Continuous learning and strategy refinement\n",
    "- **Goal-Oriented**: Pursues long-term objectives, not just immediate rewards\n",
    "\n",
    "### Experimental Modifications\n",
    "\n",
    "Try modifying the parameters to explore different behaviors:\n",
    "\n",
    "```python\n",
    "# Experiment with different risk tolerances\n",
    "run_simulation(num_games=500, risk_tolerance=0.1)  # Very conservative\n",
    "run_simulation(num_games=500, risk_tolerance=0.5)  # Moderate risk\n",
    "run_simulation(num_games=500, risk_tolerance=0.9)  # High risk\n",
    "\n",
    "# Try different game configurations\n",
    "game = BlackjackGame(num_decks=1, min_bet=10.0, max_bet=500.0)  # Single deck, high stakes\n",
    "game = BlackjackGame(num_decks=8, min_bet=1.0, max_bet=25.0)    # Casino-style, low stakes\n",
    "```\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "1. **Multi-Agent Tournaments**: Agents competing against each other\n",
    "2. **Real-Time Learning**: Online adaptation during gameplay\n",
    "3. **Explainable AI**: Visualizing the agent's decision-making process\n",
    "4. **Poker Implementation**: Extending to Texas Hold'em with opponent modeling\n",
    "5. **Human Studies**: Comparing AI performance against human experts\n",
    "\n",
    "### References\n",
    "\n",
    "- Friston, K. (2010). The free-energy principle: a unified brain theory?\n",
    "- Levin, M. (2019). The Computational Boundary of a \"Self\"\n",
    "- Parr, T. & Friston, K. (2017). Working memory, attention, and salience in active inference\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for exploring Active Inference in game playing!** üé≤üß†üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
